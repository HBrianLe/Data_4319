{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is a machine learning model that uses output probability a value that is between 0 and 1, therefore this is a binary classification model. This is a linear model that is based on classification and the probablity outcome. If the given model produces values of greater than 50%, then it will be labeled in a postive class of 1, or else it will labeled in negative class of 0. As a result for this model, we will only have two outputs of 0 or 1. \n",
    "\n",
    "![Logistic Regression](https://miro.medium.com/max/725/1*Ubge8qVlc4Xk58H1oMp4Zw.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "Exmaples of use of Logistic Regression are of:\n",
    "<ul>\n",
    "<li>Banking: What is the probability of default of failed loan? </li>\n",
    "<li>Medical: Prediction of risk of a disease.</li>\n",
    "<li>School: Passing an exam. </li>\n",
    "</ul>\n",
    "\n",
    "The classification model ouf the output of the logistic regression function is of:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat y\n",
    "= \n",
    "\\begin{cases}\n",
    "0 \\text{ if } \\hat p < 0.5        \\\\\n",
    "1 \\text{ if } \\hat p \\geqslant 0.5  \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "In this file, we look at the use of Logistic Regression applied to the [Iris Data Set](https://en.wikipedia.org/wiki/Iris_flower_data_set). The goal is to train a model that will correctly identify the flowers when we use the features on two two species of flowers.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import related packages to this project in Julia\n",
    "using CSV\n",
    "using RDatasets\n",
    "using Plots\n",
    "iris = dataset(\"datasets\", \"iris\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Logistic Regression model\n",
    "\n",
    "For our logistic regression model, we will be using a single neuron network that we will feed weights and a bias. For our Iris model, we will be using 5 total inputs, 4 from the data that we have and also 1 for the bias node. When we use the model, we are able to output a prediction of the of the probability what results in a values between 0 and 1. Together this will form a linear combination of the weights and bias in the form of:\n",
    "\n",
    "### z<sup>(i)</sup> = w<sup>T</sup>* x<sup>(i)</sup> + b\n",
    "\n",
    "This value is will not produce the value that we want of between 0 and 1. We use the sigmoid function to gain the shape and curve architecture of the logisitc regression output of 0 or 1. This will reduce the value of the z<sup>(i)</sup> to between 0 and 1.\n",
    "\n",
    "\n",
    "### Sigmoid:    σ(x) = $\\frac{1}{1+ e^-x}$\n",
    "\n",
    "By using both our linear combination and signmoid function, we can get the resulting output that we can get a prediction or a probablility value of the given input if it is classified as 0 or 1.\n",
    "### $\\hat y^i$ = σ(z<sup>(i)</sup>) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After we build our model basics, we have the next goal of building the lost function that will not only reduce the error but also reached the desired output. This is achived by looking at bernoulli binomial distribution foumula, of \n",
    "\n",
    "### p(y<sup>(i)</sup> | x<sup>(i)</sup>) = $\\hat y^i (1 - \\hat y)$<sup>1-y </sup>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### Cross Entropy Lost Function \n",
    "This is a type of loss function that will target wrong output of a model. If this logistic model, produce the wrong classification, it will change this value to adjust the weights for this value. The larger the value that is away from the correct classification, it will penalizes the probability much more than values that are much closer to the corrected output. We will go to minimize this output value to reach the desired goal by using argimin function\n",
    "\n",
    "The following is the Cross Entropy loss funciton:\n",
    "\n",
    "### L<sub>(CE)</sub>($\\hat y$, y) = - log[ p(y<sup>(i)</sup> | x<sup>(i)</sup>)]\n",
    "\n",
    "After finding the Entropy Loss Function, we can apply this function to what is known as the cost function shown below here. With out cost function, we define the error produced in our model and working with both the loss and cost, we can optimize our functions to the corrected value. This is achived by using gradiant decsent to the respective variables of weights and bias of our model. We will find partials to each one of these variables to find the minimum values for our model. \n",
    "\n",
    "### Cost Function: $\\frac{1}{N}\\sum $L<sub>(CE)</sub>($\\hat y$, y)\n",
    "###### With results in partial derivative of the weights and bias in the resulting formulas. \n",
    "\n",
    "### Weight Partial: $\\frac{1}{N}\\sum$ $\\frac{dL}{dW}$(W<sup>K+1</sup>, b<sup>K+1</sup>)\n",
    "### Bias Partial: $\\frac{1}{N}\\sum$ $\\frac{dL}{db}$(W<sup>K+1</sup>, b<sup>K+1</sup>)\n",
    "\n",
    "We can use this partical derivative to update the weights and bias for our model. The update function for our weights and bias is as follows:\n",
    "\n",
    "### Weight update: W<sup>K+1</sup> = W<sup>K</sup> - $\\alpha$ * $\\frac{1}{N}\\sum$ $\\frac{dL}{dW}$ (W<sup>K+1</sup>, b<sup>K+1</sup>)\n",
    "\n",
    "### Bias update: b<sup>K+1</sup> = b<sup>K</sup> - $\\alpha$ * $\\frac{1}{N}\\sum$ $\\frac{dL}{db}$ (W<sup>K+1</sup>, b<sup>K+1</sup>)\n",
    "\n",
    "### Lets apply the Sigmoid, Cross Entropy and Cost function to our model to train the model. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the singmoid function below, this will below\n",
    "σ(x) = 1/(1+exp(-x))\n",
    "\n",
    "#This is the log cross entropy loss\n",
    "function cross_entropy_loss(x,y,w,b)\n",
    "    \"\"\" Lost function of our model to see how far or close our model is producing models. \n",
    "    \n",
    "    Args:\n",
    "        x ([tuple]): Input of a tuple of the data. This is the x_data and the feature set as an input\n",
    "    \n",
    "        y ([tuple]): Input of a tuple of the data. This is the y_data and is the the corrected labels for our data. This is a 0 or 1 classification  \n",
    "    \n",
    "        w (Array):   An array weight input that is the size of the x tuple as above. This is the weight array size of n. \n",
    "    \n",
    "        b (array):   An array bias input that is the size of the y tuple as above. This is the bias array size of 1\n",
    "    \n",
    "    Output:\n",
    "        Returns log lost of the given point function between 0 and 1\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    return -y*log(σ(w'x + b)) - (1-y)*log(1-σ(w'x+b))\n",
    "    end \n",
    "\n",
    "#Average cost function to check after the produced model the predicted data with actual \n",
    "function average_cost(features, labels, w, b)\n",
    "    \"\"\" Fucntion to reutrn the average cost of the weights and bias in our model.\n",
    "    \n",
    "    Args:\n",
    "        x ([tuple]): Input of a tuple of the data. This is the x_data and the feature set as an input\n",
    "    \n",
    "        y ([tuple]): Input of the tuple of the data. This is the y_data and is the the corrected labels for our dataset. \n",
    "    \n",
    "        w (Array):   An array weight input that is the size of the x tuple as above. This is the weight array size of n. \n",
    "    \n",
    "        b (array):   An array bias input that is the size of the y tuple as above. This is the bias array size of 1\n",
    "    \n",
    "    Output:\n",
    "        Returns the cost value of the model with the given data\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entropy_loss(features[i], labels[i],w,b) for i = 1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "##### Types of gradient descents\n",
    "\n",
    "* Batch Gradient Descent: Whole dataset training \n",
    "* Stochastic Gradient Descent: Single random point training model\n",
    "\n",
    "Today we will be using batch gradient descent for this training model. This will mean we will train for the entire dataset.\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to do batch gradident descent. we update our weights and bias base on the \n",
    "function batch_gradient_descent(features,labels,w,b,α)\n",
    "    \"\"\" Basic one step gradient descent function that will produced updated values\n",
    "    \n",
    "    Args:\n",
    "        features ([tuple]): Input of a tuple of the data. This is the x_data and the feature set as an input\n",
    "    \n",
    "        labels ([tuple]): Input of the tuple of the data. This is the y_data and is the the corrected labels for our dataset. \n",
    "    \n",
    "        w (Array):   An array weight input that is the size of the x tuple as above. This is the weight array size of n. \n",
    "    \n",
    "        b (array):   An array bias input that is the size of the y tuple as above. This is the bias array size of 1\n",
    "        \n",
    "        α (float):  Step size of our descent. \n",
    "    \n",
    "    Output:\n",
    "        Updated Weights and bias with applied gradient descent\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    N = length(features)\n",
    "    for i = 1:N\n",
    "        del_w += (σ(w'features[i] + b) - labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i] + b) - labels[i])\n",
    "    end\n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    return w,b\n",
    "    end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting our data from the iris dataset. We will be using all the features for this one\n",
    "x_data = [[x[1], x[2], x[3],x[4]] for x in zip(iris.PetalLength[1:100],iris.PetalWidth[1:100],iris.SepalLength[1:100], iris.SepalWidth[1:100])]\n",
    "y_data = [iris.Species[i] == \"setosa\" ? 1 : 0 for i = 1:100];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.006995, -0.0026999999999999997, -0.0023249999999999994, 0.0016450000000000004], 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will test our weights and bias to get a return value to check our descent model. \n",
    "w,b = batch_gradient_descent(x_data, y_data, [0.0,0.0,0.0,0.0], 0.0, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training fucntion to execute mulitple gradient descent\n",
    "\n",
    "function train_batch_gradient_descent(features,labels, w,b,α, epochs)\n",
    "    \"\"\" Training Gradient Descent function to train our model epochs times to produce new weights and bias values\n",
    "    \n",
    "    Args:\n",
    "        features ([tuple]): Input of a tuple of the data. This is the x_data and the feature set as an input\n",
    "    \n",
    "        labels ([tuple]): Input of the tuple of the data. This is the y_data and is the the corrected labels for our dataset. \n",
    "    \n",
    "        w (Array):   An array weight input that is the size of the x tuple as above. This is the weight array size of n. \n",
    "    \n",
    "        b (array):   An array bias input that is the size of the y tuple as above. This is the bias array size of 1\n",
    "        \n",
    "        α (float):   Step size of our descent. \n",
    "    \n",
    "        epochs(int): The amount of times the gradient descent will be applied to train our model or weights and bias\n",
    "    \n",
    "    Output:\n",
    "        Output updated weights during our training to see the progress of the the changes in our data. This output will be a fraction of the epochs that we are using \n",
    "        Updated Weights and bias with applied training model over multiple gradient descent trains\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    for i = 1:epochs \n",
    "        \n",
    "        w, b = batch_gradient_descent(features, labels, w,b,α)\n",
    "        if i == 1\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/10\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/8\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/4\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/2\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs\n",
    "            println(\"Epochs \", i , \" with loss \", average_cost(x_data, y_data,w,b))\n",
    "        end\n",
    "        end \n",
    "    return w,b\n",
    "    end \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 with loss 0.6868467104604362\n",
      "Epochs 6 with loss 0.660697638157239\n",
      "Epochs 15 with loss 0.6225314728007955\n",
      "Epochs 30 with loss 0.5675813548590696\n",
      "Epochs 60 with loss 0.4782346066915051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.28426458089170487, -0.11854192838356276, 0.028566397086392296, 0.1695079181011062], 0.027141702859479727)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#below, we set our first weights and bias values and applied the model to selected data.\n",
    "w = [0.0,0.0,0.0,0.0]\n",
    "b = 0.0\n",
    "\n",
    "w,b = train_batch_gradient_descent(x_data,y_data, w,b,0.0001,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "With training our model, we want a produced average error value about .5 for us to get the best produced model. We might have to adjust both the step size and the epochs values to best find the right values to produce it. Depending on the size of the data, the step size and epochs will be diffrent from what is presented here.\n",
    "\n",
    "\n",
    "Below we create our predict model, where it will use the weights and bias to make a prediction on the input data. As said before, the logistic regression model will classify data based on the $\\hat p \\geqslant 0.5 $ or $\\hat p < 0.5 $ output values. So if a value is less than .5 it will the classifly the data to the 0 variable otherwise return a value of one as the other classification.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x,y,w,b)\n",
    "    \"\"\" Prediction function to produce classification values\n",
    "    \n",
    "    Args:\n",
    "        x ([tuple]): Input of a tuple of the data. This is the x_data and the feature set as an input\n",
    "    \n",
    "        y ([tuple]): Input of the tuple of the data. This is the y_data and is the the corrected labels for our dataset. \n",
    "    \n",
    "        w (Array):   An array weight input that is the size of the x tuple as above. This is the weight array size of n. \n",
    "    \n",
    "        b (array):   An array bias input that is the size of the y tuple as above. This is the bias array size of 1\n",
    "\n",
    "    Output:\n",
    "        1 or 0 classifaction based on our sigmoid funciton\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if σ(w'x+b) >= .5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "        end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0"
     ]
    }
   ],
   "source": [
    "#Below is our mean error result, if a value produced by our model does not equal to the actual, we will get a diffrence in values increasing our mean value\n",
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "    \n",
    "end\n",
    "print(mean_error/length(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the error of our model here is 0, which means that we did not have a value in our logistic regression model that produce an error output. \n",
    "\n",
    "___\n",
    "Next we will try to select more data to try to get an error classifcation for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[x[1], x[2], x[3],x[4]] for x in zip(iris.PetalLength[51:150],iris.PetalWidth[51:150],iris.SepalLength[51:150], iris.SepalWidth[51:150])]\n",
    "y_data = [iris.Species[i] == \"versicolor\" ? 1 : 0 for i = 51:150];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 with loss 0.6885068166534946\n",
      "Epochs 10 with loss 0.6517908263228863\n",
      "Epochs 25 with loss 0.6009811259972311\n",
      "Epochs 50 with loss 0.5326147080639443\n",
      "Epochs 100 with loss 0.43721197019824254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.9669127218190338, -0.8493355310054821, 0.6815707626396471, 0.506870035626848], 0.32280060713212333)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0.0,0.0,0.0,0.0]\n",
    "b = 0.0\n",
    "\n",
    "w,b = train_batch_gradient_descent(x_data,y_data, w,b,0.001,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04"
     ]
    }
   ],
   "source": [
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "end\n",
    "print(mean_error/length(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the selected data here, we produce a classification error of 4%. It is possible that the data here is mixed at a point that our model fails to produce a model that correctly classifies all the data at any point. \n",
    "\n",
    "___\n",
    "\n",
    "Using Logistic Regression, if we use all the features in the iris data, we have a high classifcation model for predicting and classifcation flowers. The features that went into this model were of the Petal Length and Width and the Sepal Length and Width."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
